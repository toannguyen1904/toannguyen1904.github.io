<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Toan Nguyen</title>

    <meta name="author" content="Toan Nguyen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="visual/icon.png">
</head>


<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Toan Nguyen</name>
                </p>
                <p style="text-align:center">
                    /twan Å‹wÉªn/
                </p>
                <p>
                    <font color="#E43F37">ðŸ”¥ I am actively seeking a PhD position with a research focus on the intersection of Robotics and Computer Vision.</font>
                </p>
                <p>
                    I am currently an AI Research Resident at <a href="https://fpt-aicenter.com/en">FSoft AI Center</a>, working closely with <a href="https://www.csc.liv.ac.uk/~anguyen/">Prof. Anh Nguyen</a>. My research lies in the intersection of Computer Vision, Robotics, and Generative Models.
                    Previously, I graduated with a bachelor degree in Computer Science at <a href="https://en.hcmus.edu.vn/">Ho Chi Minh City University of Science</a>.
                </p>
                <p style="text-align:center">
                    <a href="mailto:nguyentientoan190401@gmail.com"> Email </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=PhqGEY8AAAAJ"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/toannguyen1904"> Github </a> &nbsp/&nbsp
                    <a href="https://x.com/ToanNguyenCS">Twitter</a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:85%;max-width:85%" alt="profile photo" src="visual/profile.jpg">
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Thoughts on Intelligent Robots</heading>
                    <p>
                        In the long term, I envision and yearn for a world where robots assist us in every aspect of daily life.
                        As a football lover, I am especially excited about a future where robots can not only dexterously and effectively play sports like football with us, but also coach us to improve our skills.
                        A <a href="https://sites.google.com/view/competitive-robot-table-tennis/home">recent research</a> by DeepMind on table tennis has fueled my excitement even more.
                    </p>
                    <p>
                        In the short term, I believe that building world models is a critical step in vastly enriching the data needed for robot training, with generative models playing a key role.
                        The <a href="https://www.1x.tech/discover/1x-world-model">recent work</a> by 1X has given me so much renewed hope for this future.
                    </p>
                </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                <li style="margin: 5px;" >
                    <b>2024-07:</b> One paper gets accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a> as <font color="#E43F37">Oral</font> presentation.
                </li>
                <li style="margin: 5px;" >
                    <b>2024-06:</b> One paper gets accepted to <a href="https://iros2024-abudhabi.org/">IROS 2024</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2024-01:</b> Two papers get accepted to <a href="https://2024.ieee-icra.org/">ICRA 2024</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-09:</b> One paper gets accepted to <a href="https://neurips.cc/Conferences/2023">NeurIPS 2023</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-09:</b> Our paper is nominated for <font color="#E43F37">best overall paper</font> and <font color="#E43F37">best student paper awards</font> at <a href="https://ieee-iros.org/">IROS 2023</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-06:</b> One paper gets accepted to <a href="https://ieee-iros.org/">IROS 2023</a>.
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Publications</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                    <source src="visual/lgrasp6d.mp4" type="video/mp4">
                </video>
            </td>
            <td width="75%" valign="center">
                <papertitle>Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance</papertitle>
                <br>
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ">Baoru Huang</a>,
                <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ">An Vuong</a>,
                <a href="https://scholar.google.com/citations?user=NSWI3OwAAAAJ">Quan Vuong</a>,
                <a href="https://scholar.google.com/citations?user=M7T55ooAAAAJ">Dung Nguyen</a>,
                <a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ">Ngan Le</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJ">Thieu Vo</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>European Conference on Computer Vision (<strong>ECCV</strong>), 2024, <font color="#E43F37">Oral</font></em>
                <br>
                <a href="https://arxiv.org/abs/2407.13842">[arXiv]</a>
                <a href="https://airvlab.github.io/grasp-anything/">[Project]</a>
                <a href="https://github.com/Fsoft-AIC/Language-Driven-6-DoF-Grasp-Detection-Using-Negative-Prompt-Guidance">[Code]</a>
                <br>
                <p> We introduce a novel diffusion model incorporating the new concept of negative prompt guidance learning to tackle the task of 6-DoF grasp detection in cluttered point clouds. </p>
            </td>
        </tr>
        
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="visual/habicrowd.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>HabiCrowd: A High Performance Simulator for Crowd-Aware Visual Navigation</papertitle>
                <br>
                <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ">An Vuong</a>,
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ">Baoru Huang</a>,
                <a href="https://scholar.google.com/citations?user=M7T55ooAAAAJ">Dung Nguyen</a>,
                <a href="https://scholar.google.com/citations?user=vJYe5lkAAAAJ">Binh Huynh</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJ">Thieu Vo</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2306.11377">[arXiv]</a>
                <a href="https://habicrowd.github.io/">[Project]</a>
                <a href="https://github.com/Fsoft-AIC/HabiCrowd">[Code]</a>
                <br>
                <p> We introduce HabiCrowd, a new dataset and benchmark for crowd-aware visual navigation that surpasses other benchmarks in terms of human diversity and computational utilization. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="visual/3dapnet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Language-Conditioned Affordance-Pose Detection in 3D Point Clouds</papertitle>
                <br>
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ">Baoru Huang</a>,
                <a href="https://scholar.google.com/citations?user=TsjvwzgAAAAJ">Tuan Vo</a>,
                <a>Vy Truong</a>,
                <a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ">Ngan Le</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJn">Thieu Vo</a>,
                <a href="https://scholar.google.com/citations?user=UA_83MUAAAAJ">Bac Le</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2309.10911">[arXiv]</a>
                <a href="https://3dapnet.github.io/">[Project]</a>
                <a href="https://github.com/Fsoft-AIC/Language-Conditioned-Affordance-Pose-Detection-in-3D-Point-Clouds">[Code]</a>
                <br>
                <p> We address the task of language-driven affordance-pose detection in 3D point clouds. Our method simultaneously detect open-vocabulary affordances and generate affordance-specific 6-DoF poses. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="visual/openkd.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Open-Vocabulary Affordance Detection using Knowledge Distillation and Text-Point Correlation</papertitle>
                <br>
                <a href="https://scholar.google.com/citations?user=TsjvwzgAAAAJ">Tuan Vo</a>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ">Baoru Huang</a>,
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ">Ngan Le</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJn">Thieu Vo</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2309.10932">[arXiv]</a>
                <a href="https://github.com/Fsoft-AIC/Open-Vocabulary-Affordance-Detection-using-Knowledge-Distillation-and-Text-Point-Correlation">[Code]</a>
                <br>
                <p> We introduce a new open-vocabulary affordance detection method using knowledge distillation and text-point correlation. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                    <source src="visual/langscene.mp4" type="video/mp4">
                </video>
            </td>
            <td width="75%" valign="center">
                <papertitle>Language-Driven Scene Synthesis Using Multi-Conditional Diffusion Model</papertitle>
                <br>
                <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ">An Vuong</a>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=unbPvWAAAAAJ">Baoru Huang</a>,
                <a href="https://scholar.google.com/citations?user=M7T55ooAAAAJ">Dung Nguyen</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJn">Thieu Vo</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2310.15948">[arXiv]</a>
                <a href="https://lang-scene-synth.github.io/">[Project]</a>
                <a href="https://github.com/andvg3/LSDM">[Code]</a>
                <br>
                <p> We introduce Language-Driven Scene Synthesis task, which involves the leverage of human-input text prompts to generate physically plausible and semantically reasonable objects. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="visual/openad.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Open-Vocabulary Affordance Detection in 3D Point Clouds</papertitle>
                <br>
                <strong>Toan Nguyen</strong>,
                <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ">Minh Nhat Vu</a>,
                <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ">An Vuong</a>,
                <a href="https://scholar.google.com/citations?user=M7T55ooAAAAJ">Dung Nguyen</a>,
                <a href="https://scholar.google.at/citations?user=CM2qJSoAAAAJn">Thieu Vo</a>,
                <a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ">Ngan Le</a>,
                <a href="https://scholar.google.co.uk/citations?user=gEbaF0sAAAAJ">Anh Nguyen</a>
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2023, <font color="#E43F37">Best Overall & Best Student Paper Awards Finalist</font></em>
                <br>
                <a href="https://arxiv.org/abs/2303.02401">[arXiv]</a>
                <a href="https://openad2023.github.io/">[Project]</a>
                <a href="https://github.com/Fsoft-AIC/OpenAD">[Code]</a>
                <br>
                <p> Our method detects potentially unlimited textual affordance labels. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="visual/dewi.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Deep-Wide Learning Assistance for Insect Pest Classification</papertitle>
                <br>
                <strong>Toan Nguyen</strong>,
                <a>Huy Nguyen</a>,
                <a href="https://scholar.google.com.vn/citations?user=_udCkLEAAAAJ">Huy Ung</a>,
                <a>Hieu Ung</a>,
                <a href="https://scholar.google.com.vn/citations?user=dXEb3PMAAAAJ">Binh Nguyen</a>
                <br>
                <em>arXiv, 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2409.10445">[arXiv]</a>
                <a href="https://github.com/toannguyen1904/DeWi">[Code]</a>
                <br>
                <p> Our method, DeWi, can learn discriminative and in-depth features of insect pests (deep) and generalize well to a large number of categories (wide) </p>
            </td>
        </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Services</heading>
                <p>
                <li style="margin: 5px;"> Reviewer of ECCV 2024, ICRA 2024, IROS 2024</li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                </p>
            </td>
        </tr>
        </tbody></table>

    </tr>
</tbody></table>

</body>
</html>
