<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Tien Toan Nguyen</title>

    <meta name="author" content="Tien Toan Nguyen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/profile.png">
</head>


<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Zekun Qi</name>
                </p>
                <p>
                    I am a final-year MS student joint study in <a href="http://en.xjtu.edu.cn/">Xi’an Jiaotong University</a> & <a href="https://iiis.tsinghua.edu.cn/en/">IIIS, Tsinghua University</a> under the supervision of Prof. <a href="https://iiis.tsinghua.edu.cn/yao/">Andrew C. Yao</a>.
                    I collaborate closely with Prof. <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html">Kaisheng Ma</a>, Prof. <a href="https://ericyi.github.io/">Li Yi</a>, Prof. <a href="https://hughw19.github.io/">He Wang</a> and <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>.
                    <br>
                    In 2022, I obtained my bachelor’s degree in Automation from <a href="http://en.xjtu.edu.cn/"> Xi’an Jiaotong University </a>.
<!--                    <br>-->
<!--                    I am currently a research intern of Foundation Model Group at <a href="https://en.megvii.com/"> Megvii Inc</a>, where I work with <a href="https://joker316701882.github.io/"> Zheng Ge </a> and <a href="https://scholar.google.com.hk/citations?user=yuB-cfoAAAAJ"> Xiangyu Zhang </a>.-->
                </p>
                <p>
                    My research focuses on 3D Computer Vision, Multimodal Large Language Models, and Embodied AI.
                </p>
                <p style="text-align:center">
                    <a href="mailto:qizekun@gmail.com"> Email </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ap8yc3oAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/qizekun"> Github </a> &nbsp/&nbsp
                    <a href="Resume_Zekun_Qi.pdf" target="_blank"> CV </a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:85%;max-width:85%" alt="profile photo" src="images/qzk.jpg">
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                <li style="margin: 5px;" >
                    <b>2024-07:</b> One paper accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a> and one paper acccepted to <a href="https://2024.acmmm.org/">ACMMM 2024</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2024-01:</b> One paper accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> as <font color="#E43F37">Spotlight</font> presentation.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-09:</b> One paper accepted to <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-04:</b> One paper accepted to <a href="https://icml.cc/Conferences/2023">ICML 2023</a>.
                </li>
                <li style="margin: 5px;" >
                    <b>2023-01:</b> One paper accepted to <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a>.
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Publications</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/ppt24.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Positional Prompt Tuning for Efficient 3D Representation Learning</papertitle>
                <br>
                <a>Shaochen Zhang</a>*,
                <strong>Zekun Qi</strong>*,
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                <a>Xiuxiu Bai</a>,
                <a href="https://gr.xjtu.edu.cn/en/web/weixing/home">Xing Wei</a>
                <br>
                <em>arXiv preprint, 2024</em>
                <br>
                <a href="https://arxiv.org/pdf/2408.11567">[arXiv]</a>
                <a href="https://github.com/zsc000722/PPT">[Code]</a>
                <br>
                <p> We rethink the role of positional encoding in 3D representation learning, and propose Positional Prompt Tuning, a simple but efficient method for transfer learning.</p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/dreambench++24.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation</papertitle>
                <br>
                <a href="https://yuangpeng.com/">Yuang Peng</a>*,
                <a href="https://scholar.google.com/citations?user=kQucB04AAAAJ">Yuxin Cui</a>*,
                <a href="https://github.com/tanghme0w">Haomiao Tang</a>*,
                <strong>Zekun Qi</strong>,
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                <a href="https://github.com/joan-moon">Jing Bai</a>,
                <a href="https://scholar.google.com/citations?user=D6tWz44AAAAJ">Chunrui Han</a>,
                <a href="https://joker316701882.github.io/">Zheng Ge</a>,
                <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ">Xiangyu Zhang</a>,
                <a href="https://scholar.google.com/citations?user=koAXTXgAAAAJ">Shu-Tao Xia</a>
                <br>
                <em>arXiv preprint, 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2406.16855">[arXiv]</a>
                <a href="https://dreambenchplus.github.io/">[Project]</a>
                <a href="https://github.com/yuangpeng/dreambench_plus">[Code]</a>
                <br>
                <p> We collect diverse images and prompts, and utilize GPT-4o for automated evaluation aligned with human preference. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                    <source src="images/shapellm24.mp4" type="video/mp4">
                </video>
            </td>
            <td width="75%" valign="center">
                <papertitle>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</papertitle>
                <br>
                <strong>Zekun Qi</strong>,
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                <a>Shaochen Zhang</a>,
                <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
                <a href="https://scholar.google.com/citations?user=D6tWz44AAAAJ">Chunrui Han</a>,
                <a href="https://joker316701882.github.io/">Zheng Ge</a>,
                <a href="https://ericyi.github.io/">Li Yi</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>
                <br>
                <em> European Conference on Computer Vision (<strong>ECCV</strong>), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2402.17766">[arXiv]</a>
                <a href="https://qizekun.github.io/shapellm/">[Project]</a>
                <a href="https://github.com/qizekun/ShapeLLM">[Code]</a>
                <a href="https://huggingface.co/collections/qizekun/shapellm-65e978379c1260a85abe8aee">[Huggingface]</a>
                <br>
                <p> We present ShapeLLM, the first 3D Multimodal Large Language Model designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                    <source src="images/dreamllm23.mp4" type="video/mp4">
                </video>
            </td>
            <td width="75%" valign="center">
                <papertitle>DreamLLM: Synergistic Multimodal Comprehension and Creation</papertitle>
                <br>
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>*,
                <a href="https://scholar.google.com/citations?user=D6tWz44AAAAJ">Chunrui Han</a>*,
                <a href="https://yuangpeng.com/">Yuang Peng</a>,
                <strong>Zekun Qi</strong>,
                <a href="https://joker316701882.github.io/">Zheng Ge</a>,
                <a href="https://yancie-yjr.github.io/">Jinrong Yang</a>,
                <a>Liang Zhao</a>,
                <a href="https://scholar.google.com/citations?user=MVZrGkYAAAAJ">Jianjian Sun</a>,
                <a href="https://scholar.google.com/citations?user=i1PB3cEAAAAJ">Hongyu Zhou</a>,
                <a href="https://scholar.google.com/citations?user=J4naK0MAAAAJ">Haoran Wei</a>,
                <a>Xiangwen Kong</a>,
                <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ">Xiangyu Zhang</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>,
                <a href="https://ericyi.github.io/">Li Yi</a>
                <br>
                <em>International Conference on Learning Representations (<strong>ICLR</strong>), 2024, <font color="#E43F37">Spotlight</font></em>
                <br>
                <a href="https://arxiv.org/abs/2309.11499">[arXiv]</a>
                <a href="https://dreamllm.github.io/">[Project]</a>
                <a href="https://github.com/RunpeiDong/DreamLLM">[Code]</a>
                <a href="https://huggingface.co/collections/RunpeiDong/dreamllm-65fa8297e12a435e55e4b5ca">[Huggingface]</a>
                <br>
                <p> We present DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models empowered with frequently overlooked synergy between multimodal comprehension and creation. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/vpp23.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>VPP⚡: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation</papertitle>
                <br>
                <strong>Zekun Qi</strong>*,
                <a href="https://github.com/muzhou-yu">Muzhou Yu</a>*,
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>
                <br>
                <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2307.16605">[arXiv]</a>
                <a href="https://github.com/qizekun/VPP">[Code]</a>
                <a href="https://openreview.net/forum?id=etd0ebzGOG">[OpenReview]</a>
                <br>
                <p> We achieve rapid, multi-category 3D conditional generation by sharing the merits of different representations. VPP can generate 3D shapes less than 0.2s using a single RTX 2080Ti. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/pointgcc23.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast</papertitle>
                <br>
                <a href="https://github.com/Asterisci">Guofan Fan</a>,
                <strong>Zekun Qi</strong>,
                <a href="https://github.com/yibai-shi">Wenkai Shi</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>
                <br>
                <em>ACM International Conference on Multimedia (<strong>ACMMM</strong>), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2305.19623">[arXiv]</a>
                <a href="https://github.com/Asterisci/Point-GCC">[Code]</a>
                <br>
                <p> We enhance the utilization of color information to improve 3D scene self-supervised learning. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/recon23.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</papertitle>
                <br>
                <strong>Zekun Qi</strong>*,
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>*,
                <a href="https://github.com/Asterisci">Guofan Fan</a>,
                <a href="https://joker316701882.github.io/">Zheng Ge</a>,
                <a href="https://scholar.google.com.hk/citations?user=yuB-cfoAAAAJ">Xiangyu Zhang</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>,
                <a href="https://ericyi.github.io/">Li Yi</a>
                <br>
                <em>International Conference on Machine Learning (<strong>ICML</strong>), 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2302.02318">[arXiv]</a>
                <a href="https://github.com/qizekun/ReCon">[Code]</a>
                <a href="https://openreview.net/forum?id=80IfYewOh1">[OpenReview]</a>
                <br>
                <p> We propose contrast guided by reconstruct to mitigate the pattern differences between two self-supervised paradigms. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/act23.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</papertitle>
                <br>
                <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                <strong>Zekun Qi</strong>,
                <a href="https://scholar.google.com/citations?user=AK9VF30AAAAJ">Linfeng Zhang</a>,
                <a href="https://scholar.google.com/citations?user=rSP0pGQAAAAJ">Junbo Zhang</a>,
                <a href="https://scholar.google.com/citations?user=MVZrGkYAAAAJ">Jianjian Sun</a>,
                <a href="https://joker316701882.github.io/">Zheng Ge</a>,
                <a href="https://ericyi.github.io/">Li Yi</a>,
                <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html/">Kaisheng Ma</a>
                <br>
                <em>International Conference on Learning Representations (<strong>ICLR</strong>), 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2212.08320">[arXiv]</a>
                <a href="https://github.com/RunpeiDong/ACT">[Code]</a>
                <a href="https://openreview.net/forum?id=8Oun8ZUVe8N">[OpenReview]</a>
                <br>
                <p> We propose to use autoencoders as cross-modal teachers to transfer dark knowledge into 3D representation learning. </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <p>
                <li style="margin: 5px;"> 2022 Outstanding Graduate, Xi’an Jiaotong University</li>
                <li style="margin: 5px;"> 2021 <a href="https://mp.weixin.qq.com/s/nSirUtlo1j87JKXvJ9BttA"> Annual Spiritual Civilization Award </a>, Xi’an Jiaotong University</li>
                <li style="margin: 5px;"> 2020 <a href="https://mp.weixin.qq.com/s/2J32r3O4VsJnre3ckPPXrQ"> National runner-up </a> of the China Undergraduate Physics Tournament (CUPT) as the team leader </li>
                <li style="margin: 5px;"> 2019 Chen Qi Scholarship, Xi’an Jiaotong University</li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                </p>
            </td>
        </tr>
        </tbody></table>

    </tr>
</tbody></table>

<p>
    <div class="center" id="clustrmaps-widget" style="width:8%"><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=kSH52UUctXHgyZTvsD38_H_styzXoRq6qG_cg4tttWI">&copy; Zekun Qi | Last updated: Aug 6, 2023</script></div>
    <br>
    <div style="text-align:center;"> &copy; Zekun Qi | Last updated: Jul 16, 2024 </div>
</p>

</body>
</html>
